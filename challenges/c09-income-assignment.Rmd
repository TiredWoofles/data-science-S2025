---
title: "US Income"
author: "(Suwanee Li)"
date: 2025-4-6
output:
  github_document:
    toc: true
prerequisites:
  - e-stat09-bootstrap
---

*Purpose*: We've been learning how to quantify uncertainty in estimates through the exercises; now its time to put those skills to use studying real data. In this challenge we'll use concepts like confidence intervals to help us make sense of census data.

*Reading*: - [Using ACS Estimates and Margin of Error](https://www.census.gov/data/academy/webinars/2020/calculating-margins-of-error-acs.html) (Optional, see the PDF on the page) - [Patterns and Causes of Uncertainty in the American Community Survey](https://www.sciencedirect.com/science/article/pii/S0143622813002518?casa_token=VddzQ1-spHMAAAAA:FTq92LXgiPVloJUVjnHs8Ma1HwvPigisAYtzfqaGbbRRwoknNq56Y2IzszmGgIGH4JAPzQN0) (Optional, particularly the *Uncertainty in surveys* section under the Introduction.)

<!-- include-rubric -->

# Grading Rubric

<!-- -------------------------------------------------- -->

Unlike exercises, **challenges will be graded**. The following rubrics define how you will be graded, both on an individual and team basis.

## Individual

<!-- ------------------------- -->

| Category | Needs Improvement | Satisfactory |
|------------------------|------------------------|------------------------|
| Effort | Some task **q**'s left unattempted | All task **q**'s attempted |
| Observed | Did not document observations, or observations incorrect | Documented correct observations based on analysis |
| Supported | Some observations not clearly supported by analysis | All observations clearly supported by analysis (table, graph, etc.) |
| Assessed | Observations include claims not supported by the data, or reflect a level of certainty not warranted by the data | Observations are appropriately qualified by the quality & relevance of the data and (in)conclusiveness of the support |
| Specified | Uses the phrase "more data are necessary" without clarification | Any statement that "more data are necessary" specifies which *specific* data are needed to answer what *specific* question |
| Code Styled | Violations of the [style guide](https://style.tidyverse.org/) hinder readability | Code sufficiently close to the [style guide](https://style.tidyverse.org/) |

## Submission

<!-- ------------------------- -->

Make sure to commit both the challenge report (`report.md` file) and supporting files (`report_files/` folder) when you are done! Then submit a link to Canvas. **Your Challenge submission is not complete without all files uploaded to GitHub.**

# Setup

<!-- ----------------------------------------------------------------------- -->

```{r setup}
library(tidyverse)
```

### **q1** Load the population data from c06; simply replace `filename_pop` below.

```{r q1-task}
## TODO: Give the filename for your copy of Table B01003
filename_pop <- "./6_data/ACSDT5Y2018.B01003-Data.csv"

## NOTE: No need to edit
df_pop <-
  read_csv(
    filename_pop,
    skip = 1,
  ) %>% 
  rename(
    population_estimate = `Estimate!!Total`
  )

df_pop
```

You might wonder why the `Margin of Error` in the population estimates is listed as `*****`. From the [documentation (PDF link)](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwj81Omy16TrAhXsguAKHTzKDQEQFjABegQIBxAB&url=https%3A%2F%2Fwww2.census.gov%2Fprograms-surveys%2Facs%2Ftech_docs%2Faccuracy%2FMultiyearACSAccuracyofData2018.pdf%3F&usg=AOvVaw2TOrVuBDlkDI2gde6ugce_) for the ACS:

> If the margin of error is displayed as ‘\*\*\*\*\*’ (five asterisks), the estimate has been controlled to be equal to a fixed value and so it has no sampling error. A standard error of zero should be used for these controlled estimates when completing calculations, such as those in the following section.

This means that for cases listed as `*****` the US Census Bureau recommends treating the margin of error (and thus standard error) as zero.

### **q2** Obtain median income data from the Census Bureau:

-   `Filter > Topics > Income and Poverty > Income and Poverty`
-   `Filter > Geography > County > All counties in United States`
-   Look for `Median Income in the Past 12 Months` (Table S1903)
-   Download the 2018 5-year ACS estimates; save to your `data` folder and add the filename below.

```{r q2-task}
## TODO: Give the filename for your copy of Table S1903
filename_income <- "./9_data/ACSST5Y2018.S1903-Data.csv"

## NOTE: No need to edit
df_income <-
  read_csv(filename_income, skip = 1)
df_income
```

Use the following test to check that you downloaded the correct file:

```{r q2-tests}
## NOTE: No need to edit, use to check you got the right file.
assertthat::assert_that(
  df_income %>%
    filter(Geography == "0500000US01001") %>%
    pull(`Estimate!!Percent Distribution!!FAMILY INCOME BY FAMILY SIZE!!2-person families`)
  == 45.6
)

print("Well done!")
```

This dataset is in desperate need of some *tidying*. To simplify the task, we'll start by considering the `\\d-person families` columns first.

### **q3** Tidy the `df_income` dataset by completing the code below. Pivot and rename the columns to arrive at the column names `id, geographic_area_name, category, income_estimate, income_moe`.

*Hint*: You can do this in a single pivot using the `".value"` argument and a `names_pattern` using capture groups `"()"`. Remember that you can use an OR operator `|` in a regex to allow for multiple possibilities in a capture group, for example `"(Estimate|Margin of Error)"`.

```{r q3-task}
df_q3 <-
  df_income %>%
  select(
    Geography,
    contains("Geographic"),
    # This will select only the numeric d-person family columns;
    # it will ignore the annotation columns
    contains("median") & matches("\\d-person families") & !contains("Annotation of")
  ) %>%
  mutate(across(contains("median"), ~ suppressWarnings(as.numeric(.)))) %>% 
  # suppressWarnings to avoid error when submitting
## TODO: Pivot the data, rename the columns
  pivot_longer(
    cols = -c(Geography, `Geographic Area Name`),
    names_to = c(".value", "category"),
    names_pattern = "(Estimate|Margin of Error)!!.*!!(\\d-person families)"
  ) %>%
  rename(
    geographic_area_name = `Geographic Area Name`,
    income_estimate = Estimate,
    income_moe = `Margin of Error`
  ) %>%
  glimpse()
```

Use the following tests to check your work:

```{r q3-tests}
## NOTE: No need to edit
assertthat::assert_that(setequal(
  names(df_q3),
  c("Geography", "geographic_area_name", "category", "income_estimate", "income_moe")
))

assertthat::assert_that(
  df_q3 %>%
    filter(Geography == "0500000US01001", category == "2-person families") %>%
    pull(income_moe)
  == 6663
)

print("Nice!")
```

The data gives finite values for the Margin of Error, which is closely related to the Standard Error. The Census Bureau documentation gives the following relationship between Margin of Error and Standard Error:

$$\text{MOE} = 1.645 \times \text{SE}.$$

### **q4** Convert the margin of error to standard error. Additionally, compute a 99% confidence interval on income, and normalize the standard error to `income_CV = income_SE / income_estimate`. Provide these columns with the names `income_SE, income_lo, income_hi, income_CV`.

```{r q4-task}
df_q4 <- df_q3 %>%
  mutate(
    # Convert Margin of Error (MOE) to Standard Error (SE)
    # MOE = 1.645 * SE rearranged is SE = MOE / 1.645
    income_SE = income_moe / 1.645,
    
    # 99% CI (z* = 2.576 for 99% CI)
    income_lo = income_estimate - 2.576 * income_SE,
    income_hi = income_estimate + 2.576 * income_SE,
    
    # Compute Coefficient of Variation (CV)
    income_CV = income_SE / income_estimate
  ) %>%
  # numeric columns 
  mutate(across(c(income_SE, income_lo, income_hi, income_CV), as.numeric))

df_q4

```

Use the following tests to check your work:

```{r q4-tests}
## NOTE: No need to edit
assertthat::assert_that(setequal(
  names(df_q4),
  c("Geography", "geographic_area_name", "category", "income_estimate", "income_moe",
    "income_SE", "income_lo", "income_hi", "income_CV")
))

assertthat::assert_that(
  abs(
    df_q4 %>%
    filter(Geography == "0500000US01001", category == "2-person families") %>%
    pull(income_SE) - 4050.456
  ) / 4050.456 < 1e-3
)

assertthat::assert_that(
  abs(
    df_q4 %>%
    filter(Geography == "0500000US01001", category == "2-person families") %>%
    pull(income_lo) - 54513.72
  ) / 54513.72 < 1e-3
)

assertthat::assert_that(
  abs(
    df_q4 %>%
    filter(Geography == "0500000US01001", category == "2-person families") %>%
    pull(income_hi) - 75380.28
  ) / 75380.28 < 1e-3
)

assertthat::assert_that(
  abs(
    df_q4 %>%
    filter(Geography == "0500000US01001", category == "2-person families") %>%
    pull(income_CV) - 0.06236556
  ) / 0.06236556 < 1e-3
)

print("Nice!")
```

One last wrangling step: We need to join the two datasets so we can compare population with income.

### **q5** Join `df_q4` and `df_pop`.

```{r q5-task}
## TODO: Join df_q4 and df_pop by the appropriate column
df_data <- df_q4 %>%
  left_join(df_pop, by = "Geography") %>%
  select(-contains("Annotation"))  # Drop annotation columns present
df_data
```

# Analysis

<!-- ----------------------------------------------------------------------- -->

We now have both estimates and confidence intervals for `\\d-person families`. Now we can compare cases with quantified uncertainties: Let's practice!

### **q6** Study the following graph, making sure to note what you can *and can't* conclude based on the estimates and confidence intervals. Document your observations below and answer the questions.

```{r q6-task}
## NOTE: No need to edit; run and inspect
wid <- 0.5

df_data %>%
  filter(str_detect(geographic_area_name, "Massachusetts")) %>%
  mutate(
    county = str_remove(geographic_area_name, " County,.*$"),
    county = fct_reorder(county, income_estimate)
  ) %>%

  ggplot(aes(county, income_estimate, color = category)) +
  geom_errorbar(
    aes(ymin = income_lo, ymax = income_hi),
    position = position_dodge(width = wid)
  ) +
  geom_point(position = position_dodge(width = wid)) +

  coord_flip() +
  labs(
    x = "County",
    y = "Median Household Income"
  )
```

**Observations**:

-   Document your observations here.
    -   Nantucket's 5-person median income has the largest standard error which we can tell due to how large it's error bar is.
    -   Suffolk appears to shows the closest median household income regardless of the family size as with a small standard error so a higher degree of confidence and the values are very close to each other.
    -   2-person appear to for the most part have the lowest median household income as they consistently are the lowest for each country with usually a pretty small error bar.
-   Can you confidently distinguish between household incomes in Suffolk county? Why or why not?
    -   The values that the median values in Suffolk county with their range of values from their error bars heavily overlap. It would be hard to distinguish as many of the values could be any of the family sizes.
-   Which counties have the widest confidence intervals?
    -   Nantucket, Berkshire, Dukes, Hampshire, Franklin, Barnstable all have wide intervals in this

In the next task, you'll investigate the relationship between population and uncertainty.

### **q7** Plot the standard error against population for all counties. Create a visual that effectively highlights the trends in the data. Answer the questions under *observations* below.

*Hint*: Remember that standard error is a function of *both* variability (e.g. variance) and sample size.

```{r q7-task}
ggplot(df_data, aes(x = population_estimate, y = income_SE)) +
  geom_point(alpha = 0.3, color = "steelblue") +  # Semi-transparent points
  geom_smooth(method = "loess", color = "red", se = FALSE) +  # Trend line
  scale_x_log10(labels = scales::comma) +  # Log-scale for population
  labs(
    title = "Income Standard Error vs. County Population",
    x = "Population (log scale)",
    y = "Standard Error of Median Income",
    caption = "Source: 2018 ACS 5-Year Estimates"
  ) +
  theme_minimal()
```

**Observations**:

-   What *overall* trend do you see between `SE` and population? Why might this trend exist?
    -   As the population increases, the standard error (SE) of median income decreases. This trend exists because, when the population size increases, a sample that is proportional to the population (for example, about 10%) will also be larger. Larger sample sizes lead to more stable estimates of the median income, reducing the variability in the sample and thus lowering the standard error. In smaller populations, where a proportionally smaller sample is taken, individual data points have a larger effect on the estimate, increasing the standard error.
-   What does this *overall* trend tell you about the relative ease of studying small vs large counties?
    -   It's harder to study smaller counties as they have a greater amount of standard error making it more difficult to have a high degree of confidence in the median value someone analyzing may find.

# Going Further

<!-- ----------------------------------------------------------------------- -->

Now it's your turn! You have income data for every county in the United States: Pose your own question and try to answer it with the data.

### **q8** Pose your own question about the data. Create a visualization (or table) here, and document your observations.

```{r q8-task}
## TODO: Pose and answer your own question about the data
library(ggplot2)
library(dplyr)
library(forcats)

df_data %>%
  # Clean county names by removing state refs
  mutate(
    county_state = str_remove(geographic_area_name, " County"),
    # Order family sizes 
    category = fct_relevel(category,
                          "2-person families",
                          "3-person families",
                          "4-person families",
                          "5-person families",
                          "6-person families")
  ) %>%
  
  ggplot(aes(x = category, y = income_estimate, color = category)) +
  # Add error bars for confidence interval
  geom_errorbar(
    aes(ymin = income_lo, ymax = income_hi),
    width = 0.2,
    position = position_dodge(width = wid),
    alpha = 0.7
  ) +
  geom_point(
    position = position_dodge(width = wid),
    size = 2.5
  ) +
  # Mean reference line
  stat_summary(
    fun = mean, 
    geom = "crossbar", 
    width = 0.6,
    color = "gray30",
    show.legend = FALSE
  ) +
  labs(
    title = "Median Income by Family Size with Confidence Intervals",
    subtitle = "Showing 99% confidence ranges across all counties",
    x = "Family Size",
    y = "Median Income (USD)",
    color = "Family Size"
  ) +
  scale_color_brewer(palette = "Set1") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
    legend.position = "none"
  )
```

**Observations**:

-   3-person median families seem to have the smallest error bar sizes and so the highest degree of confidence
-   The median income does not seem affected too greatly by the family size as the mean reference line seem very close to each other in value
-   Across all the family sizes, it does appear that a large majority of the data regardless of county have a singular range of where their values lie resulting in the data looking like blocks

Ideas:

-   Compare trends across counties that are relevant to you; e.g. places you've lived, places you've been, places in the US that are interesting to you.
-   In q3 we tidied the median `\\d-person families` columns only.
    -   Tidy the other median columns to learn about other people groups.
    -   Tidy the percentage columns to learn about how many households of each category are in each county.
-   I was curious to see if across the larger US if family size has an impact on the median household income. It appears that it does not. This makes me think about how larger families tend to have hamidowns likely because there are more people to take care of but the income remains the same. Also, i've read how in larger families what tends to happen are the parents are really busy and the eldest children frequently care for their siblings and when there are a lot of kids a lot of them can feel neglected because it's harder to get undivided attention and according to this graph money too.

# References

<!-- ----------------------------------------------------------------------- -->

[1] Spielman SE, Folch DC, Nagle NN (2014) Patterns and causes of uncertainty in the American Community Survey. Applied Geography 46: 147–157. <pmid:25404783> [link](https://www.sciencedirect.com/science/article/pii/S0143622813002518?casa_token=VddzQ1-spHMAAAAA:FTq92LXgiPVloJUVjnHs8Ma1HwvPigisAYtzfqaGbbRRwoknNqZ6Y2IzszmGgIGH4JAPzQN0)
